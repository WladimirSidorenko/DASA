#!/usr/bin/env python3

"""Train and predict sentiment scores using XLNET model.

"""

##################################################################
# Imports
from argparse import ArgumentParser
from dasa.dl import DATALOADER_KWARGS
from dasa.utils.common import LOGGER
from scipy.special import softmax
from sklearn.model_selection import KFold
from typing import List, Tuple
from torch import tensor
from torch.optim import AdamW
from torch.utils.data import DataLoader, TensorDataset
from tqdm import trange
from transformers import XLNetTokenizer, XLNetForSequenceClassification
import json
import numpy as np
import sys
import torch


##################################################################
# Types
Indices = List[int]
Scores = List[float]

##################################################################
# Variables and Constants
DEVICE = torch.device("cuda:0" if torch.cuda.is_available()
                      else "cpu")
DFLT_MODEL_NAME = "xlnet-base-cased"
TRAIN = "train"
TEST = "test"
# we might not have the neutral class, but still need all available classes to
# be contiguous
LABEL2PROB = {
    "negative": 0,
    "positive": 1,
    "neutral": 2
}
EPOCHS = 4
MAX_SEQ_LENGTH = 512


##################################################################
# Methods
def tokenize_doc(tokenizer: XLNetTokenizer,
                 doc: dict) -> Tuple[Indices, List[Indices]]:
    """Convert document's and edus' text into a tensor of word ids.

    """
    text = tokenizer.encode(doc["text"], add_special_tokens=True)
    edus = [
        tokenizer.encode(
            [doc["toks"][t]["form"] for t in edu_i["toks"]],
            add_special_tokens=True
        )
        for edu_i in doc["edus"]
    ]
    return text, edus


def pad(X: List[Indices], pad_token_id: int) -> tensor:
    """Pad lists of token ids to the maximum observed length.

    """
    max_length = min(MAX_SEQ_LENGTH,
                     max(len(x_i) for x_i in X))
    for i, x_i in enumerate(X):
        if len(x_i) > max_length:
            X[i] = x_i[:max_length]
        else:
            x_i += [pad_token_id] * (max_length - len(x_i))
    return tensor(X)


def read_data(fnames: List[str],
              tokenizer: XLNetTokenizer) -> Tuple[tensor, tensor, tensor,
                                                  dict, dict]:
    """Generate data set from JSON files.

    """
    X_txt = []
    X_edu = []
    Y = []
    txt_id2doc_id = {}
    doc_id2edu_ids = {}
    # convert documents and EDUs into token ids
    for fname_i in fnames:
        with open(fname_i) as ifile:
            data = json.load(ifile)
            for doc_i in data["docs"]:
                doc_id = doc_i["doc_id"]
                x_txt, x_edus = tokenize_doc(tokenizer, doc_i)
                txt_id2doc_id[len(X_txt)] = doc_id
                # training will be performed only on full text
                X_txt.append(x_txt)
                y_i = LABEL2PROB[doc_i["label"]]
                Y.append(y_i)
                # whereas predictions will also be performed on EDUs
                edu_start_idx = len(X_edu)
                edu_end_idx = edu_start_idx + len(x_edus)
                doc_id2edu_ids[doc_id] = list(
                    range(edu_start_idx, edu_end_idx)
                )
                X_edu.extend(x_edus)
    X_txt = pad(X_txt, tokenizer.pad_token_id)
    X_edu = pad(X_edu, tokenizer.pad_token_id)
    Y = tensor(Y)
    return X_txt, X_edu, Y, txt_id2doc_id, doc_id2edu_ids


def accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)


def init_optimizer(model):
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer
                    if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer
                    if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
    return AdamW(optimizer_grouped_parameters, lr=2e-5)


def train(model: XLNetForSequenceClassification, X: tensor, Y: tensor):
    """Optimize model on given data.

    """
    dataset = TensorDataset(X, Y)
    data = DataLoader(dataset, **DATALOADER_KWARGS)
    optimizer = init_optimizer(model)
    model.train()
    for e_i in trange(EPOCHS, desc="Epoch"):
        epoch_loss = 0.
        for (x, y) in data:
            optimizer.zero_grad()
            outputs = model(x.to(DEVICE), labels=y.to(DEVICE))
            loss = outputs[0]
            epoch_loss += loss.item()
            # Backward pass
            loss.backward()
            # Update parameters and take a step using the computed gradient
            optimizer.step()
        LOGGER.info("Epoch %d: training loss: %f", e_i, epoch_loss)


def predict(model: XLNetForSequenceClassification, X: tensor) -> np.array:
    result = []
    # as `X` might be too big, we divide it into batches
    dataset = TensorDataset(X)
    data = DataLoader(dataset)
    model.eval()
    with torch.no_grad():
        for (x,) in data:
            y = model(x.to(DEVICE))
            result.extend(y[0].numpy())
    return np.vstack(result)


def finalize_prediction(Y):
    """Add missing columns, perform softmax, and convert prediction to list.

    """
    Y = softmax(Y, axis=1)
    # fill missing column for the neutral class
    if Y.shape[-1] == 2:
        Y = np.insert(Y, 1, 0, axis=1)
    # otherwise, swap first and second columns
    else:
        Y[:, [1, 2]] = Y[:, [2, 1]]
    return Y.tolist()


def cross_val_predict(X_txt, X_edu, Y,
                      txt_id2doc_id,
                      doc_id2edu_ids) -> Tuple[np.array, np.array]:
    """Perform 5-fold cross-validation, fine-tuning the model on `X_txt`.

    Args:
      analyzer (SentimentAnalyzer): sentiment analyzer
      messages (list[dict]): messages to analyze
      txt_id2doc_id (dict): mapping from running number of examples to their
        document id's
      doc_id2edu_ids (dict): mapping from document id's to the running numbers
        of EDUs

    Returns:
      void:

    Note:
      modifies `tweets` in place

    """
    num_labels = Y.max().item() + 1
    Y_txt = np.zeros((len(X_txt), num_labels))
    Y_edu = np.zeros((len(X_edu), num_labels))

    kfold = KFold(shuffle=True)
    for i, (train_ids, test_ids) in enumerate(kfold.split(X_txt, Y)):
        model = XLNetForSequenceClassification.from_pretrained(
            DFLT_MODEL_NAME, num_labels=num_labels
        )
        model.to(DEVICE)
        train(model, X_txt[train_ids], Y[train_ids])
        Y_txt[test_ids] = predict(model, X_txt[test_ids])
        doc_ids = [txt_id2doc_id[txt_id] for txt_id in test_ids]
        edu_ids = [edu_id
                   for doc_id in doc_ids
                   for edu_id in doc_id2edu_ids[doc_id]]
        Y_edu[edu_ids] = predict(model, X_edu[edu_ids])
    return (finalize_prediction(Y_txt),
            finalize_prediction(Y_edu))


def add_scores(fnames: List[str], Y_txt: np.array, Y_edus: np.array,
               doc_id2txt_id: dict, doc_id2edu_ids: dict):
    """Add polarity predictions to messages and EDUs.

    """
    for fname_i in fnames:
        with open(fname_i) as ifile:
            data = json.load(ifile)
        # add polarity scores
        for doc_i in data["docs"]:
            doc_id = doc_i["doc_id"]
            txt_id = doc_id2txt_id[doc_id]
            doc_i["polarity_scores"]["xlnet"] = Y_txt[txt_id]
            edu_ids = doc_id2edu_ids[doc_id]
            edus = doc_i["edus"]
            for j, edu_id in enumerate(edu_ids):
                edus[j]["polarity_scores"]["xlnet"] = Y_edus[edu_id]
        # write polarity scores
        with open(fname_i, 'w') as ofile:
            json.dump(data, ofile)


def main(argv):
    """Main script for adding polarity scores to tweets and EDUs.

    Args:
      argv (list[str]): CLI arguments

    Retruns:
      int: 0 on success, non-0 otherwise

    """
    argparser = ArgumentParser(
        description="Script for adding polarity scores to tweets and EDUs."
    )
    argparser.add_argument("json_files",
                           nargs='+',
                           help="JSON file containing tweets")
    args = argparser.parse_args(argv)
    tokenizer = XLNetTokenizer.from_pretrained(DFLT_MODEL_NAME)
    X_txt, X_edus, Y, txt_id2doc_id, doc_id2edu_ids = read_data(
        args.json_files, tokenizer
    )
    Y_txt, Y_edu = cross_val_predict(X_txt, X_edus, Y,
                                     txt_id2doc_id, doc_id2edu_ids)
    doc_id2txt_id = {doc_id: txt_id
                     for txt_id, doc_id in txt_id2doc_id.items()}
    add_scores(args.json_files, Y_txt, Y_edu,
               doc_id2txt_id, doc_id2edu_ids)
    return 0


##################################################################
# Main
if __name__ == "__main__":
    main(sys.argv[1:])
