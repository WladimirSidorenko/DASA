#!/usr/bin/env python
# -*- mode: python; coding: utf-8; -*-

##################################################################
# Imports
from __future__ import absolute_import, unicode_literals, print_function

from argparse import ArgumentParser
import json
import logging
import sys

from dasa import (DASBaseAnalyzer, DDRAnalyzer, DUSAnalyzer,
                  LastAnalyzer, RootAnalyzer, R2N2Analyzer,
                  WangAnalyzer, HCRFAnalyzer)
from dasa.utils.common import LOGGER
from dasa.constants import (DFLT_MODEL_PATH,
                            LAST, ROOT, NO_DISCOURSE, R2N2, DDR, WANG,
                            HCRF, BHATIA, CHENLO, HEERSCHOP, PCC, ZHOU,
                            POSITIVE, NEGATIVE, NEUTRAL)


##################################################################
# Variables and Constants
DEBUG = "debug"
TRAIN = "train"
TEST = "test"
NO_REL_TYPE = (LAST, NO_DISCOURSE)


##################################################################
# Methods
def _add_cmn_options(parser):
    """Add common options to option subparser

    Args:
      parser (argparse.ArgumentParser):
        option subparser to which new options should be added

    Returns:
      void:

    """
    parser.add_argument("-m", "--model",
                        help="path to the main model (if different from"
                        " default)", type=str, default=DFLT_MODEL_PATH)
    parser.add_argument("files", help="input file(s)",
                        type=str, nargs="+")


def _read_data(files):
    """Read files and return an iterator over tweets.

    Args:
      files (list[str]): list of input files

    """
    if files is None:
        raise StopIteration
    for fname in files:
        with open(fname, 'r') as ifile:
            data = json.load(ifile)
            if "tweets" not in data:
                LOGGER.warn("No tweets found in %s", fname)
                continue
            for tweet_i in data["tweets"]:
                if len(tweet_i["edus"]) < 2:
                    LOGGER.warn(
                        "Skipping tweet %s which contains only one EDU.",
                        tweet_i["msg_id"]
                    )
                    continue
                elif tweet_i["label"] not in (POSITIVE, NEGATIVE, NEUTRAL):
                    LOGGER.warn("Skipping tweet %s due to non-standard"
                                " polarity (%s).",
                                tweet_i["msg_id"], tweet_i["label"])
                    continue
                yield tweet_i


def main(argv):
    """Main method for training and applying discourse-aware sentiment classifiers.

    Args:
      argv (list[str]): CLI arguments

    Returns:
      int: 0 on success, non-0 otherwise

    """
    argparser = ArgumentParser(
        description="Train or test discourse-aware sentiment analyzer.")
    argparser.add_argument("-v", "--verbose", help="debug mode",
                           action="store_true")

    subparsers = argparser.add_subparsers(
        help="operation to perform", dest="mode"
    )

    parser_train = subparsers.add_parser(
        TRAIN, help="train model on the provided data")
    parser_train.add_argument("-d", "--dev",
                              help="development data", action="append")
    parser_train.add_argument("-g", "--grid-search",
                              help="determine optimal parameters using grid"
                              " search", action="store_true")
    parser_train.add_argument("-r", "--relation-scheme",
                              help="RST relation scheme to be analyzed",
                              choices=(BHATIA, CHENLO, HEERSCHOP, PCC, ZHOU))
    parser_train.add_argument("-t", "--type",
                              help="type(s) of the model(s) to train",
                              choices=(DDR, LAST, ROOT, R2N2, WANG,
                                       NO_DISCOURSE, HCRF),
                              required=True, type=str)
    _add_cmn_options(parser_train)

    parser_test = subparsers.add_parser(
        TEST, help="determine polarity of the given messages")
    _add_cmn_options(parser_test)

    parser_debug = subparsers.add_parser(
        DEBUG, help="explain model's prediction"
    )
    _add_cmn_options(parser_debug)
    args = argparser.parse_args()

    if args.verbose or args.mode == DEBUG:
        log_lvl = logging.DEBUG
        LOGGER.setLevel(log_lvl)
        for handler_i in LOGGER.handlers:
            handler_i.setLevel(log_lvl)

    if args.mode == TRAIN:
        if args.type not in NO_REL_TYPE and args.relation_scheme is None:
            LOGGER.error("No relation scheme specified for model %s",
                         args.type)
            sys.exit(2)

        train_set = [tweet_i for tweet_i in _read_data(args.files)]
        LOGGER.debug("Reading development set...")
        dev_set = [tweet_i for tweet_i in _read_data(args.dev)]
        LOGGER.debug("Initializing analyzer...")
        if args.type == DDR:
            analyzer = DDRAnalyzer(args.relation_scheme)
        elif args.type == LAST:
            analyzer = LastAnalyzer()
        elif args.type == NO_DISCOURSE:
            analyzer = DUSAnalyzer()
        elif args.type == ROOT:
            analyzer = RootAnalyzer(args.relation_scheme)
        elif args.type == R2N2:
            analyzer = R2N2Analyzer(args.relation_scheme)
        elif args.type == WANG:
            analyzer = WangAnalyzer(args.relation_scheme)
        elif args.type == HCRF:
            analyzer = HCRFAnalyzer(args.relation_scheme)
        else:
            raise NotImplementedError(
                "Analyzer {:s} has not been implemented yet.".format(
                    args.type
                )
            )
        LOGGER.debug("Training analyzer...")
        analyzer.train(train_set, dev_set,
                       grid_search=args.grid_search)
        LOGGER.debug("Analyzer trained.")
        analyzer.save(args.model)
    else:
        analyzer = DASBaseAnalyzer.load(args.model)
        if args.mode == DEBUG:
            predict = analyzer.debug
        else:
            predict = analyzer.predict

        ret = []
        for tweet_i in _read_data(args.files):
            tweet_i["predicted"] = predict(tweet_i)
            ret.append(tweet_i)
        json.dump({"tweets": ret},
                  sys.stdout, indent=1, separators=[',', ": "])


##################################################################
# Main
if __name__ == "__main__":
    main(sys.argv[1:])
